{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONVb972yb1EisIReRnlxln",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/up2113232/up2113232_coursework/blob/dev/Q2_folder/Q2_NN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fdc1f3c"
      },
      "source": [
        "\n",
        "# Gaming Mental Health Prediction with Neural Networks\n",
        "## Predicting GAD_T, SWL_T, and SPIN_T Scores from Gaming Behavior\n",
        "\n",
        "This notebook demonstrates how to build a neural network to predict mental health scores\n",
        "(GAD_T = Anxiety, SWL_T = Life Satisfaction, SPIN_T = Social Phobia) based on gaming habits.\n",
        "\n",
        "**Dataset:** Online Gaming Anxiety Data from Kaggle\n",
        "**Target Variables:** GAD_T, SWL_T, SPIN_T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first we have to import all of our important libraries that will be used."
      ],
      "metadata": {
        "id": "MEIT2RNNmChu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import sys # Used for system-specific parameters and functions, like modifying the Python path\n",
        "import os  # Provides a way of using operating system dependent functionality, like file paths\n",
        "\n",
        "# Add parent directory to path to import our functions\n",
        "# This line ensures Python can find our custom 'functions.py' file, which likely contains\n",
        "# helper functions for data cleaning, encoding, splitting, and scaling.\n",
        "sys.path.append('..')\n",
        "\n",
        "# Core data manipulation and visualisation libraries\n",
        "# pandas is crucial for handling data in tables (DataFrames)\n",
        "import pandas as pd\n",
        "# numpy is used for numerical operations, especially with arrays\n",
        "import numpy as np\n",
        "# matplotlib is a foundational library for creating static, interactive, and animated visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "# seaborn is built on matplotlib and provides a high-level interface for drawing attractive statistical graphics\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and preprocessing libraries from scikit-learn\n",
        "# train_test_split: for dividing data into training and testing sets\n",
        "# cross_val_score: for evaluating model performance using cross-validation\n",
        "# GridSearchCV: for hyperparameter tuning (finding the best parameters for a model)\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "# StandardScaler: for standardizing features by removing the mean and scaling to unit variance\n",
        "# LabelEncoder: for converting categorical labels into numerical format\n",
        "# MinMaxScaler: for scaling features to a given range, usually between zero and one\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "# mean_squared_error, mean_absolute_error, r2_score: metrics to evaluate regression model performance\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "# MLPRegressor: Multi-layer Perceptron regressor, a type of neural network for regression tasks\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "# MultiOutputRegressor: a strategy for fitting a single regressor on multiple targets\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "# System and warnings\n",
        "# This line is used to ignore warning messages that might clutter the output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "XKkGpzHlmKGy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will load the functions we will use in this notebook from our functions file"
      ],
      "metadata": {
        "id": "aJfvk4wPmyt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From our custom 'functions.py' file, we import specific helper functions.\n",
        "# These functions encapsulate common data processing steps, making our main notebook cleaner and more organized.\n",
        "# - clean_data: likely handles initial data cleaning (e.g., duplicates, basic missing value handling).\n",
        "# - encode_features: converts non-numerical (categorical) data into numerical format that machine learning models can understand.\n",
        "# - split_data: divides the dataset into training and testing sets.\n",
        "# - scale_features: normalizes or standardizes numerical features.\n",
        "from functions import clean_data, encode_features, split_data, scale_features"
      ],
      "metadata": {
        "id": "_SnxDnaSm3_J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to load our data as a pandas data frame so we can manipulate it easier."
      ],
      "metadata": {
        "id": "vhoPdD8MmUUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file into a pandas DataFrame.\n",
        "# The 'encoding' parameter is specified to handle potential character encoding issues in the file.\n",
        "# We use a try-except block to gracefully handle the case where the file might not be found.\n",
        "try:\n",
        "  df = pd.read_csv('gaming_anxiety_data.csv', encoding='ISO-8859-1')\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\" File not found! Please upload your dataset first.\")"
      ],
      "metadata": {
        "id": "8sUeqOKomiY8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns that will be used as input features for our Neural Network.\n",
        "# These are the independent variables (X) that the network will use to make predictions.\n",
        "feature_columns = ['GADE', 'Game', 'Hours', 'earnings', 'whyplay',\n",
        "                   'streams', 'Narcissism', 'Gender',\n",
        "                   'Age', 'Work', 'Playstyle']\n",
        "\n",
        "# Define the target columns. These are the dependent variables (y) we want to predict.\n",
        "# Our network will try to learn the relationship between 'feature_columns' and 'target_columns'.\n",
        "target_columns = ['GAD_T', 'SWL_T', 'SPIN_T']\n",
        "\n",
        "# Create a new DataFrame 'df' containing only the selected feature and target columns.\n",
        "# .copy() is used to ensure we are working with a separate copy of the data,\n",
        "# preventing unintended modifications to the original DataFrame loaded earlier.\n",
        "df = df[feature_columns + target_columns].copy()"
      ],
      "metadata": {
        "id": "okE9L0XSn6uW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the 'clean_data' function (imported from functions.py) to handle initial data cleaning steps.\n",
        "# This function typically removes duplicate rows and might display information about missing values.\n",
        "# The cleaned data is stored in a new DataFrame called 'df_cleaned_initial'.\n",
        "print(\"Cleaning dataset...\")\n",
        "df_cleaned_initial = clean_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ELoo6vtoGo6",
        "outputId": "61f5bec0-64ba-4103-cb7d-d30cfb84a048"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning dataset...\n",
            "Missing values per column:\n",
            "GADE          649\n",
            "Game            0\n",
            "Hours          30\n",
            "earnings        0\n",
            "whyplay         0\n",
            "streams       100\n",
            "Narcissism     23\n",
            "Gender          0\n",
            "Age             0\n",
            "Work           38\n",
            "Playstyle       0\n",
            "GAD_T           0\n",
            "SWL_T           0\n",
            "SPIN_T        650\n",
            "dtype: int64\n",
            "Removed 0 rows with missing values\n",
            "Removed 51 duplicate rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will encode the string values (categorical features) into corresponding numbers using 'encode_features'.\n",
        "# This is necessary because Neural Networks typically require numerical inputs.\n",
        "df_encoded = encode_features(df_cleaned_initial)\n",
        "\n",
        "# We will then clean up any remaining missing values by dropping rows that contain NaN (Not a Number).\n",
        "# This ensures that our final dataset 'df_clean' is entirely numerical and free of missing data.\n",
        "df_clean = df_encoded.dropna()\n",
        "\n",
        "# Print a summary of missing values to confirm the cleaning process was successful.\n",
        "print(f\"\\n Original Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\" Missing values after cleaning and encoding: {df_clean.isnull().sum().sum()}\")\n",
        "# If there are still missing values, this block will print which columns contain them.\n",
        "if df_clean.isnull().sum().sum() > 0:\n",
        "    print(\"Columns with missing values in df_clean:\")\n",
        "    print(df_clean.isnull().sum()[df_clean.isnull().sum() > 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrq140JCoPgE",
        "outputId": "7da2ef46-a50c-47cf-8ea8-2b5025c7fead"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Original Missing values: 1490\n",
            " Missing values after cleaning and encoding: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to split and scale our data, just like in Q1 as Neural networks require the same format of data and we will use a 80/20 train/test split for our NN"
      ],
      "metadata": {
        "id": "t2uRvAwCp19r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the features (X) from the target variables (y).\n",
        "# X contains the input data, and y contains the values we want to predict.\n",
        "X = df_clean[feature_columns]\n",
        "y = df_clean[target_columns]\n",
        "\n",
        "# Define the proportion of data to be used for testing (20%) and a random state for reproducibility.\n",
        "test_size = 0.2\n",
        "random_state = 42\n",
        "\n",
        "# Split the data into training and testing sets using the split_data function.\n",
        "# X_train, y_train are used to train the model.\n",
        "# X_test, y_test are used to evaluate the model's performance on unseen data.\n",
        "X_train, X_test, y_train, y_test = split_data(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "# Initialize StandardScaler to normalize the feature data.\n",
        "# Scaling is important for neural networks as it helps with faster convergence and better performance.\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler on the training features and then transform both training and test features.\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize another StandardScaler for the target variables.\n",
        "# It's good practice to scale target variables as well when using regression models.\n",
        "target_scaler = StandardScaler()\n",
        "# Fit and transform the training target variables.\n",
        "y_train_scaled = target_scaler.fit_transform(y_train)\n",
        "# Transform the test target variables (using the scaler fitted on training targets).\n",
        "y_test_scaled = target_scaler.transform(y_test)"
      ],
      "metadata": {
        "id": "MrW40g8LqDPv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Multi-layer Perceptron (MLP) Regressor, which is a type of neural network.\n",
        "model = MLPRegressor(\n",
        "    #Defines the architecture of the hidden layers. (100, 50, 25) means three\n",
        "    # hidden layers with 100, 50, 25 neurons respectively.\n",
        "    hidden_layer_sizes=(100, 50, 25),\n",
        "    #The activation function for the hidden layers. 'relu' (Rectified Linear Unit) is a common choice.\n",
        "    activation='relu',\n",
        "    #The algorithm for weight optimization. 'adam' is an efficient stochastic optimizer.\n",
        "    solver='adam',\n",
        "    #The size of minibatches for stochastic optimizers. 32 is a common batch size.\n",
        "    batch_size=32,\n",
        "    # learning_rate: How the learning rate is adjusted over time. 'adaptive' decreases it when validation score stops improving.\n",
        "    learning_rate='adaptive',\n",
        "    # learning_rate_init: The initial learning rate.\n",
        "    learning_rate_init=0.001,\n",
        "    # max_iter: The maximum number of epochs (iterations over the entire training data).\n",
        "    max_iter=50,\n",
        "    # random_state: Ensures reproducibility of the results.\n",
        "    random_state=42,\n",
        "    # verbose: If True, prints progress messages to stdout.\n",
        "    verbose=True,\n",
        "    # early_stopping: Whether to use early stopping to terminate training when validation score is not improving.\n",
        "    early_stopping=False,\n",
        "    # validation_fraction: The proportion of training data to set aside as validation set for early stopping.\n",
        "    validation_fraction=0.1,\n",
        "    # n_iter_no_change: Maximum number of epochs to not meet `tol` improvement before early stopping.\n",
        "    n_iter_no_change=20\n",
        ")"
      ],
      "metadata": {
        "id": "mzsA1vvLrd3Q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf077ef"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Now we will train our neural network using the scaled training data (`X_train_scaled`, `y_train_scaled`). The model will learn patterns from this data to make predictions."
      ]
    }
  ]
}